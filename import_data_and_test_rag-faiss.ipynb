{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was part of testing different approaches for an LLM chatbot for first-responders to natural disasters using Llama with RAG. The goal was to include location and disaster-specific documents in various forms (eg, after-action reports from previous disasters in the area, local disaster protocols, vulnerable population locations) in the RAG vector store, then allow first responders to query the LLM during an actual disaster to quickly help them make decisions (identify next steps, decide areas to target, etc). The ultimate tool would be built and hosted in AWS, but packaged and downloadable locally to enable responders to use it in the field running on their laptops if wifi was not available. \n",
    "\n",
    "In this script, I was doing part of the testing for a local version of the tool - it is not in a final, production-clean state, as we were producing this proof-of-concept for a grant, but the result of this script is a functioning local RAG LLM (tested using the code in the final cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# in terminal, need to run:\n",
    "# pip install llama-index-vector-stores-faiss, langchain-huggingface, faiss-cpu\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#from pymilvus import MilvusClient, DataType, utility\n",
    "from llama_index.core import SimpleDirectoryReader, Settings\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, ServiceContext\n",
    "#from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "import faiss\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import cProfile, pstats\n",
    "from pstats import SortKey\n",
    "from datetime import datetime\n",
    "\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\natra\\\\Documents\\\\CrisisReady\\\\Meta Grant'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files into Milvus collection\n",
    "source_dir = 'C:\\\\Users\\\\natra\\\\Documents\\\\CrisisReady\\\\Meta Grant\\\\rag-data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
    "#model = SentenceTransformer(model_id)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_id)\n",
    "#tokenizer = embeddings.tokenizer\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    " # https://www.reddit.com/r/LangChain/comments/16m1nee/not_being_able_to_use_huggingfaceembedding_from/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-22 17:49:14.728195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Advanced encoding /KSCms-UHC-H not implemented yet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-22 18:16:50.212803\n"
     ]
    }
   ],
   "source": [
    "nidm_data = source_dir + \"NIDM Case Studies\\\\\"\n",
    "\n",
    "# loading when plugged in with only 100 watt power, not full power, took 24 min to load the 65 pdfs in NIDM Case Studies \n",
    "# (max 161,000 KB, min 600 KB)\n",
    "print(datetime.now())\n",
    "loader=PyPDFDirectoryLoader(nidm_data)\n",
    "docs = loader.load()\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-22 18:16:50.219953\n",
      "2024-06-22 18:16:50.627938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13509"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llamaindex chunk size is 1024, overlap default is 20, so use these\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "print(datetime.now())\n",
    "doc_chunks = text_splitter.split_documents(docs)\n",
    "print(datetime.now())\n",
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_chunks_lst = [doc for doc in doc_chunks]\n",
    "# after 4+ hours, with no completion and a significant drain on my computer's resources, stopped the process\n",
    "print(datetime.now())\n",
    "db = FAISS.from_documents(embedding=embeddings,documents=doc_chunks)\n",
    "print(datetime.now())\n",
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_local_loc = source_dir + \"nidm_case_studies_faiss\"\n",
    "db.save_local(faiss_local_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collection_name': 'meta_proto', 'auto_id': False, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}, 'is_primary': True}, {'field_id': 101, 'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1024}}], 'aliases': [], 'collection_id': 449918578127474310, 'consistency_level': 0, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True}\n",
      "[{'count(*)': 10308}]\n"
     ]
    }
   ],
   "source": [
    "# 1. Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=\"http://localhost:19530\"\n",
    ")\n",
    "\n",
    "res = client.describe_collection(\n",
    "    collection_name=\"meta_proto\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "\n",
    "res = client.query(\n",
    "    collection_name=\"meta_proto\",\n",
    "    output_fields = [\"count(*)\"]\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JINA_AI_API_KEY='jina_1ebacc7cdda8421fba6a677bdbc16ff4-lrMzYOnPMf2Rp_hOoNDkoWutkp2'\n",
    "#\n",
    "#embeddings = JinaEmbeddings(\n",
    "#   jina_api_key=JINA_AI_API_KEY, model_name=\"jina-embeddings-v2-small-en\",\n",
    "#    embed_batch_size=16\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.7.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "C:\\Users\\natra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# can choose Embeddings model based on HuggingFace MTEB leaderboard\n",
    "# https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "# First attempted Alibaba-NLP/gte-large-en-v1.5 on 5/21/2024 as highest-ranking model for Retrieval \n",
    "# that was small enough to run relatively easily on local machines\n",
    "# however, received ValueError: Loading Alibaba-NLP/gte-large-en-v1.5 requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.\n",
    "\n",
    "# Next, attempted Snowflake/snowflake-arctic-embed-l as relatively small (< 500 mil params) next-highest-performer\n",
    "# trying to increase embedding batch sizes for speed, suggested on git issues page - requires more memory\n",
    "embeddings = HuggingFaceEmbedding(model_name=\"Snowflake/snowflake-arctic-embed-l\",embed_batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model=embeddings\n",
    "Settings.llm = Ollama(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-05 14:55:47.716071\n",
      "2024-06-05 14:56:21.786331\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "print(datetime.now())\n",
    "# to load initial set of documents, from South Asia Disasters PDFs, NIDM Case Studies folders, as well as\n",
    "# AIDMI's Fact Finding Report - Cyclone Michaung Minor Text Edits.pdf, Cyclone Michaung Infographic - April 1 2024.pptx.pdf\n",
    "# these took 10 min to load into documents object with SimpleDirectoryReader\n",
    "#documents = SimpleDirectoryReader(source_dir,recursive=True).load_data(num_workers=10)\n",
    "\n",
    "# to load next batch of documents, India cyclone Meta mobility data as csvs\n",
    "# took 4 min to all load\n",
    "# took 2 min to load data without mobility/ folder - VectorStoreIndex was getting hung up \n",
    "# took 40 sec to load a single population-density csv\n",
    "#meta_data = source_dir + \"meta_cyclone_mobility_india_2024\\\\\"\n",
    "nidm_data = source_dir + \"NIDM Case Studies\\\\\"\n",
    "documents = SimpleDirectoryReader(nidm_data,recursive=True).load_data(num_workers=10)\n",
    "print(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0cba0a29-94d1-4d14-a28b-61dbb7f9f6d3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].doc_id)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MilvusDB_local = \"C:\\\\Users\\\\natra\\\\milvus\"\n",
    "# this uri is for the Milvus DB running on Docker Compose\n",
    "vector_store = MilvusVectorStore(address=\"localhost:19530\",\n",
    "                                 collection_name=\"meta_proto\",\n",
    "                                 # use dim of embedding? Or \n",
    "                                 dim=1024,\n",
    "                                 # use overwrite if you want to replace existing DB;\n",
    "                                 # once loaded, I won't overwrite for now\n",
    "                                 #overwrite=True\n",
    "                                 )\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use when loading new documents into vector store\n",
    "# PDFs in initial load took 3.25 hrs\n",
    "\n",
    "# for non-mobility meta data, after 04:53 hrs, was estimating another 14:36 hours just for the first step 'Parsing nodes'.\n",
    "# taking about 3 gb ram, 20+ % of my CPU \n",
    "\n",
    "# for single population-density csv, took 54 min\n",
    "\n",
    "# for 2 pop density csvs, with embed_batch_size increased to 20, getting up to 100% cpu utilization (across all cores), was running for 53 min\n",
    "# and was starting the 2nd generating embeddings process, likely to be another ~50 min to finalize\n",
    "print(datetime.now())\n",
    "index = VectorStoreIndex.from_documents(\n",
    "                    documents, embed_model=embeddings, storage_context=storage_context,\n",
    "                    show_progress=True)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use when have docs in vector store and just need to query\n",
    "index=VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "print(datetime.now())\n",
    "response = query_engine.query(\"There is a monsoon approaching Tezpur, Assam. What are the key humanitarian needs which local organizations need to be aware of based on the history of response to monsoons in this area? And please contextualize this in terms of any policy advisory on best practices for monsoon respose. Thanks -- my boss will execute me if you don't get this exactly right\")\n",
    "print(textwrap.fill(str(response), 100))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FROM HERE BELOW JUST SIMPLE EXAMPLE, with lite Milvus database\n",
    "#### uses same embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simple example using data loaded directly from docs\n",
    "source_dir = 'C:\\\\Users\\\\natra\\\\Documents\\\\CrisisReady\\\\Meta Grant\\\\rag-data\\\\'\n",
    "#file = source_dir + \"AIDMI's Fact Finding Report - Cyclone Michaung Minor Text Edits.pdf\"\n",
    "file = source_dir + \"Cyclone Michaung Infographic - April 1 2024.pptx.pdf\"\n",
    "#file = source_dir + \"worldpop_ucdb_stats_india.csv\"\n",
    "\n",
    "loader = PyPDFLoader(file)\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "vector_store = Milvus.from_documents(documents=all_splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "callback_manager=CallbackManager(\n",
    "            [StreamingStdOutCallbackHandler()]\n",
    "),\n",
    "stop=[\"<|eot_id|>\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query:  How should Chennai prepare for the next cyclone?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natra\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I don't have specific information on how Chennai should prepare for the next cyclone. However, it's important for the city to prioritize disaster risk reduction and management by leveraging data and technology to enhance preparedness and response efforts, as acknowledged in the data acknowledgement section. This could involve initiatives such as improving early warning systems, strengthening infrastructure, and providing support to vulnerable communities like slums, which are a significant proportion of Chennai's population (refer to Figure 2).{'query': 'How should Chennai prepare for the next cyclone?', 'result': \"Based on the provided context, I don't have specific information on how Chennai should prepare for the next cyclone. However, it's important for the city to prioritize disaster risk reduction and management by leveraging data and technology to enhance preparedness and response efforts, as acknowledged in the data acknowledgement section. This could involve initiatives such as improving early warning systems, strengthening infrastructure, and providing support to vulnerable communities like slums, which are a significant proportion of Chennai's population (refer to Figure 2).\"}\n"
     ]
    }
   ],
   "source": [
    "query = input(\"\\nQuery: \")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")   \n",
    "    \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=vector_store.as_retriever(), chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# result limited by the limited number of context documents I stored in the RAG locally for testing\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
